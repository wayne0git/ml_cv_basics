{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo_v5_example_opencv.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPtcx5y7SSQ1A/hRKjWlHxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wayne0git/ml_cv_basics/blob/master/object_detection/yolo_v5_example_opencv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO v5 Example\n",
        "## https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/"
      ],
      "metadata": {
        "id": "huRU4DlndDaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & Convert Model (Pytorch => ONNX)"
      ],
      "metadata": {
        "id": "BDgeJs-6dHiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository. \n",
        "!git clone https://github.com/ultralytics/YOLOv5"
      ],
      "metadata": {
        "id": "IPG78st8dQf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "%cd YOLOv5\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "QajX7Qf3dVkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download .pt model.\n",
        "!wget https://github.com/ultralytics/YOLOv5/releases/download/v6.1/YOLOv5s.pt"
      ],
      "metadata": {
        "id": "EuKcWC55dnUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert .pt model to ONNX and TF Lite\n",
        "!python export.py --weights YOLOv5s.pt --include onnx tflite"
      ],
      "metadata": {
        "id": "A0oJtOXNd2yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyYMpgZcws5"
      },
      "outputs": [],
      "source": [
        "# Download the file.\n",
        "from google.colab import files\n",
        "files.download('YOLOv5s.onnx')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run OpenCV DNN Inference"
      ],
      "metadata": {
        "id": "CmPwxoC2fUWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Library"
      ],
      "metadata": {
        "id": "d41bmaTjfXnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install new OpenCV to support inference\n",
        "!pip install opencv-python==4.5.1.48"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3wjUTtZjuvz",
        "outputId": "749d54c0-1d2c-49aa-ffef-d872d35250d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/lib/python3.7/dist-packages (4.5.1.48)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.5.1.48) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HZBwQ-fKfWS0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter"
      ],
      "metadata": {
        "id": "5hq09ZUufmH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants.\n",
        "INPUT_WIDTH = 640\n",
        "INPUT_HEIGHT = 640\n",
        "SCORE_THRESHOLD = 0.1\n",
        "NMS_THRESHOLD = 0.1\n",
        "CONFIDENCE_THRESHOLD = 0.1"
      ],
      "metadata": {
        "id": "-n540XGYfolL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text parameters.\n",
        "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
        "FONT_SCALE = 0.7\n",
        "THICKNESS = 1"
      ],
      "metadata": {
        "id": "mOLKpJjXfluL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colors.\n",
        "BLACK  = (0,0,0)\n",
        "BLUE   = (255,178,50)\n",
        "YELLOW = (0,255,255)"
      ],
      "metadata": {
        "id": "ONw0EOSjfroT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load class names.\n",
        "classesFile = \"coco.names\"\n",
        "classes = None\n",
        "with open(classesFile, 'rt') as f:\n",
        "    classes = f.read().rstrip('\\n').split('\\n')"
      ],
      "metadata": {
        "id": "yCacmf5Vip_d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre/Post Processing Function"
      ],
      "metadata": {
        "id": "WHQ_s425fzEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_label(im, label, x, y):\n",
        "    \"\"\"Draw text onto image at location.\"\"\"\n",
        "    # Get text size.\n",
        "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
        "    dim, baseline = text_size[0], text_size[1]\n",
        "\n",
        "    # Use text size to create a BLACK rectangle.\n",
        "    cv2.rectangle(im, (x,y), (x + dim[0], y + dim[1] + baseline), (0,0,0), cv2.FILLED)\n",
        "\n",
        "    # Display text inside the rectangle.\n",
        "    cv2.putText(im, label, (x, y + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)"
      ],
      "metadata": {
        "id": "OoCWMldXf1dp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return detection result of (1, 25200, 85)\n",
        "# Each row is an bounding box\n",
        "# 85 columns : x, y, w, h, conf, 80 class scores\n",
        "def pre_process(input_image, net):\n",
        "      # Create a 4D blob from a frame.\n",
        "      blob = cv2.dnn.blobFromImage(input_image, 1/255,  (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
        "\n",
        "      # Sets the input to the network.\n",
        "      net.setInput(blob)\n",
        "\n",
        "      # Run the forward pass to get output of the output layers.\n",
        "      outputs = net.forward(net.getUnconnectedOutLayersNames())\n",
        "\n",
        "      return outputs"
      ],
      "metadata": {
        "id": "jwrCukgKgHSX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process(input_image, outputs):\n",
        "      # Lists to hold respective values while unwrapping.\n",
        "      class_ids = []\n",
        "      confidences = []\n",
        "      boxes = []\n",
        "\n",
        "      # Rows.\n",
        "      rows = outputs[0].shape[1]\n",
        "      image_height, image_width = input_image.shape[:2]\n",
        "\n",
        "      # Resizing factor.\n",
        "      x_factor = image_width / INPUT_WIDTH\n",
        "      y_factor =  image_height / INPUT_HEIGHT\n",
        "\n",
        "      # Iterate through 25200 detections.\n",
        "      for r in range(rows):\n",
        "            row = outputs[0][0][r]\n",
        "            confidence = row[4]\n",
        "\n",
        "            # Discard bad detections and continue.\n",
        "            if confidence >= CONFIDENCE_THRESHOLD:\n",
        "                  classes_scores = row[5:]\n",
        "                  # Get the index of max class score.\n",
        "                  class_id = np.argmax(classes_scores)\n",
        "\n",
        "                  #  Continue if the class score is above threshold.\n",
        "                  if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
        "                        confidences.append(confidence)\n",
        "                        class_ids.append(class_id)\n",
        "                        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
        "                        left = int((cx - w/2) * x_factor)\n",
        "                        top = int((cy - h/2) * y_factor)\n",
        "                        width = int(w * x_factor)\n",
        "                        height = int(h * y_factor)\n",
        "                        box = np.array([left, top, width, height])\n",
        "                        boxes.append(box)\n",
        "      \n",
        "      # Perform non maximum suppression to eliminate redundant, overlapping boxes with lower confidences.\n",
        "      indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
        "\n",
        "      for i in indices:\n",
        "        box = boxes[i]\n",
        "        left = box[0]\n",
        "        top = box[1]\n",
        "        width = box[2]\n",
        "        height = box[3]             \n",
        "        # Draw bounding box.             \n",
        "        cv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 3*THICKNESS)\n",
        "        # Class label.                      \n",
        "        label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])             \n",
        "        # Draw label.             \n",
        "        draw_label(input_image, label, left, top)\n",
        "\n",
        "      return input_image                        "
      ],
      "metadata": {
        "id": "2zjhZxAsg8Tk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Inference"
      ],
      "metadata": {
        "id": "b8fSyMyAjBiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image.\n",
        "frame = cv2.imread('test.jpg')\n",
        "\n",
        "# Give the weight files to the model and load the network\n",
        "modelWeights = \"YOLOv5s.onnx\"\n",
        "net = cv2.dnn.readNet(modelWeights)\n",
        "\n",
        "# Process image.\n",
        "detections = pre_process(frame, net)\n",
        "img = post_process(frame.copy(), detections)\n",
        "\n",
        "# Show image\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "6nE_3dBojBL7",
        "outputId": "2ada7d11-2814-412a-de7e-3d6385877044"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2e13a8a1d55e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Give the weight files to the model and load the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodelWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"YOLOv5s.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QAGSbc-SkN0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}