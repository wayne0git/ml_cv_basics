{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alpr_example_darknet_paddleocr.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0UCD4mfMh5o2zEVDO1LH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wayne0git/ml_cv_basics/blob/master/alpr/alpr_example_darknet_paddleocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALPR Example\n",
        "- https://learnopencv.com/automatic-license-plate-recognition-using-deep-learning/"
      ],
      "metadata": {
        "id": "SdvxLY2sQJ3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Prepartion"
      ],
      "metadata": {
        "id": "UpgDAmJDskRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Darknet"
      ],
      "metadata": {
        "id": "Q3hR6N_4rN81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AlexeyAB/darknet"
      ],
      "metadata": {
        "id": "Y3aOz0mIrQDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
        "!sed -i 's/LIBSO=0/LIBSO=1/' Makefile"
      ],
      "metadata": {
        "id": "7fnsvzfGrTrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make darknet (builds darknet so that the darknet executable file can be used to run or train object detectors)\n",
        "!make"
      ],
      "metadata": {
        "id": "tVD-6-nTrVfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PaddleOCR"
      ],
      "metadata": {
        "id": "17_REbwBpsaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paddlepaddle-gpu\n",
        "!pip install \"paddleocr>=2.0.1\""
      ],
      "metadata": {
        "id": "ByQ7aIpdprjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSORT tracker\n",
        "- Download pretrained deep association metric model called `mars-small128.pb`, can be downloaded from [here](https://drive.google.com/drive/folders/1n0jB3zwJysi6YDi4n0HVKz5yOZ0eNA2B?usp=sharing)"
      ],
      "metadata": {
        "id": "ucjj0SlKs50M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nwojke/deep_sort.git"
      ],
      "metadata": {
        "id": "NZrfTazVQarb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepSORT modification\n",
        "- The original DeepSORT repo uses a deprecated sklearn function called `linear_assignment`, which needs to be replaced for error free execution of code with scipy.\n",
        "- 1. Open ./deep_sort/deep_sort/linear_assignment.py\n",
        "- 2. Replace `from sklearn.utils.linear_assignment_ import linear_assignment` in line 4 with `from scipy.optimize import linear_sum_assignment`.\n",
        "\n",
        "- 3. Replace `indices = linear_assignment(cost_matrix)` in line 58 with the following lines of code:\n",
        "```\n",
        "  indices = linear_sum_assignment(cost_matrix)\n",
        "  indices = np.asarray(indices)\n",
        "  indices = np.transpose(indices)\n",
        "```\n",
        "- 4. Also, rename ./deep_sort/tools as ./deep_sort/tools_deepsort to avoid any name overlapping.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AIVvBLTuog1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "iFwTkXydp6P9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Library"
      ],
      "metadata": {
        "id": "b9ok_uxcp8UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import uuid\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "CNQ0KmlWryrF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Q6GxRr3bsClN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSORT\n",
        "%cd /content/deep_sort\n",
        "from application_util import preprocessing\n",
        "from deep_sort import nn_matching\n",
        "from deep_sort.detection import Detection\n",
        "from deep_sort.tracker import Tracker\n",
        "from tools_deepsort import generate_detections as gdet"
      ],
      "metadata": {
        "id": "RQZ_dmQUoj4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from paddleocr import PaddleOCR"
      ],
      "metadata": {
        "id": "EhPtN7ZsqL5F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Darknet\n",
        "%cd /content/darknet\n",
        "\n",
        "import darknet\n",
        "from darknet_images import load_images\n",
        "from darknet_images import image_detection"
      ],
      "metadata": {
        "id": "yDl3DtPYru4N",
        "outputId": "6984f47b-7278-4a7a-8db0-fdf1add10008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/darknet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter"
      ],
      "metadata": {
        "id": "xUrAxJYBsbCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv4 model files\n",
        "# Files from https://github.com/wayne0git/ml_cv_basics/blob/master/alpr/license_plate_detection_train_yolov4.ipynb\n",
        "config_file = '/content/yolov4-obj.cfg'\n",
        "data_file = '/content/obj.data'\n",
        "weights = '/content/yolov4-obj_best.weights'"
      ],
      "metadata": {
        "id": "L4lOEwCBsdFV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv4 param\n",
        "batch_size = 1\n",
        "thresh = 0.6"
      ],
      "metadata": {
        "id": "noU2_TRKs4xk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSORT model files\n",
        "deepsort_model_file = '/content/mars-small128.pb'"
      ],
      "metadata": {
        "id": "pzBZElCRuOCG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables storing colors and fonts.\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "blue_color = (255,0,0)\n",
        "white_color = (255,255,255)\n",
        "black_color = (0,0,0)\n",
        "green_color = (0,255,0)\n",
        "yellow_color = (178, 247, 218)"
      ],
      "metadata": {
        "id": "0x1faRdPsccd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize PaddleOCR"
      ],
      "metadata": {
        "id": "qM1PxlWtq90m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ocr = PaddleOCR(lang='en',rec_algorithm='CRNN')"
      ],
      "metadata": {
        "id": "lu3r6eD0qMWA",
        "outputId": "61bce88e-8b8e-40f2-8829-5a8526e1db6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022/04/23 10:00:32] ppocr DEBUG: Namespace(alpha=1.0, benchmark=False, beta=1.0, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_thresh=0.9, cpu_threads=10, crop_res_save_dir='./output', det=True, det_algorithm='DB', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_fce_box_type='poly', det_limit_side_len=960, det_limit_type='max', det_model_dir='/root/.paddleocr/whl/det/en/en_ppocr_mobile_v2.0_det_infer', det_pse_box_thresh=0.85, det_pse_box_type='quad', det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_polygon=False, det_sast_score_thresh=0.5, draw_img_save_dir='./inference_results', drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, fourier_degree=5, gpu_mem=500, help='==SUPPRESS==', image_dir=None, ir_optim=True, label_list=['0', '180'], lang='en', layout_label_map=None, layout_path_model='lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config', max_batch_size=10, max_text_length=25, min_subgraph_size=15, mode='structure', ocr_version='PP-OCRv2', output='./output', precision='fp32', process_id=0, rec=True, rec_algorithm='CRNN', rec_batch_num=6, rec_char_dict_path='/usr/local/lib/python3.7/dist-packages/paddleocr/ppocr/utils/en_dict.txt', rec_image_shape='3, 32, 320', rec_model_dir='/root/.paddleocr/whl/rec/en/en_number_mobile_v2.0_rec_infer', save_crop_res=False, save_log_path='./log_output/', scales=[8, 16, 32], show_log=True, structure_version='STRUCTURE', table_char_dict_path=None, table_char_type='en', table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=False, use_dilation=False, use_gpu=True, use_mp=False, use_onnx=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Function"
      ],
      "metadata": {
        "id": "yYsmug8utK98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop(image, coord):\n",
        "  # Cropping is done by -> image[y1:y2, x1:x2].\n",
        "  cr_img = image[int(coord[1]):int(coord[3]), int(coord[0]):int(coord[2])]\n",
        "  return cr_img"
      ],
      "metadata": {
        "id": "SWfwbj37tMvj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_bbox(detections, out_size, in_size):\n",
        "  coord = []\n",
        "  scores = []\n",
        "\n",
        "  # Scaling the bounding boxes to the different size\n",
        "  for det in detections:\n",
        "    points = list(det[2])\n",
        "    conf = det[1]\n",
        "    xmin, ymin, xmax, ymax = darknet.bbox2points(points)\n",
        "    y_scale = float(out_size[0]) / in_size[0]\n",
        "    x_scale = float(out_size[1]) / in_size[1]\n",
        "    ymin = int(y_scale * ymin)\n",
        "    ymax = int(y_scale * ymax)\n",
        "    xmin = int(x_scale * xmin) if int(x_scale * xmin) > 0 else 0\n",
        "    xmax = int(x_scale * xmax)\n",
        "    final_points = [xmin, ymin, xmax-xmin, ymax-ymin]\n",
        "    scores.append(conf)\n",
        "    coord.append(final_points)\n",
        "  return coord, scores"
      ],
      "metadata": {
        "id": "GFK_rDxhtPJe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_det(frame, config_file, data_file, batch_size, weights, threshold, output, network, class_names, class_colors, save = False, out_path = ''):\n",
        "\n",
        "  prev_time = time.time()\n",
        "  \n",
        "  # Preprocessing the input image.\n",
        "  width = darknet.network_width(network)\n",
        "  height = darknet.network_height(network)\n",
        "  darknet_image = darknet.make_image(width, height, 3)\n",
        "  image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  image_resized = cv2.resize(image_rgb, (width, height))\n",
        "  \n",
        "  # Passing the image to the detector and store the detections\n",
        "  darknet.copy_image_from_bytes(darknet_image, image_resized.tobytes())\n",
        "  detections = darknet.detect_image(network, class_names, darknet_image, thresh=threshold)\n",
        "  darknet.free_image(darknet_image)\n",
        "\n",
        "  # Plotting the deetections using darknet in-built functions\n",
        "  image = darknet.draw_boxes(detections, image_resized, class_colors)\n",
        "  print(detections)\n",
        "  if save:\n",
        "    im = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    file_name = out_path + str(uuid.uuid4()) +'-det.jpg'\n",
        "    cv2.imwrite(os.path.join(output, file_name), im)\n",
        "\n",
        "  # Calculating time taken and FPS for detection\n",
        "  det_time = time.time() - prev_time\n",
        "  fps = int(1/(time.time() - prev_time))\n",
        "  print(\"Detection time: {}\".format(det_time))\n",
        "  \n",
        "  # Resizing predicted bounding box from 416x416 to input image resolution\n",
        "  out_size = frame.shape[:2]\n",
        "  in_size = image_resized.shape[:2]\n",
        "  if detections:\n",
        "    coord, scores = resize_bbox(detections, out_size, in_size)\n",
        "    return coord, scores, det_time\n",
        "  else:\n",
        "    scores = 0\n",
        "    return detections,scores, det_time "
      ],
      "metadata": {
        "id": "GFEDslnvtcHM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_ocr(preds, rec_conf, ocr_res, track_id):\n",
        "  for info in preds:\n",
        "    # Check if it is current track id\n",
        "    if info['track_id'] == track_id:\n",
        "      # Check if the ocr confidenence is maximum or not\n",
        "      if info['ocr_conf'] < rec_conf:\n",
        "        info['ocr_conf'] = rec_conf\n",
        "        info['ocr_txt'] = ocr_res\n",
        "      else:\n",
        "        rec_conf = info['ocr_conf']\n",
        "        ocr_res = info['ocr_txt']\n",
        "      break\n",
        "  return preds, rec_conf, ocr_res"
      ],
      "metadata": {
        "id": "PODyWmxVtiZ8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference function"
      ],
      "metadata": {
        "id": "9pyCWZWutnLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_img(input, config_file, weights, out_path):\n",
        "  # Loading darknet network and classes along with the bbox colors.\n",
        "  network, class_names, class_colors = darknet.load_network(\n",
        "            config_file,\n",
        "            data_file,\n",
        "            weights,\n",
        "            batch_size= batch_size\n",
        "        )\n",
        "  \n",
        "  # Reading the image and performing YOLOv4 detection. \n",
        "  img = cv2.imread(input)\n",
        "  bboxes, scores, det_time = yolo_det(img, config_file, data_file, batch_size, weights, thresh, out_path, network, class_names, class_colors)\n",
        "\n",
        "  # Extracting or cropping the license plate and applying the OCR.\n",
        "  for bbox in bboxes:\n",
        "    bbox = [bbox[0], bbox[1], bbox[2] + bbox[0], bbox[3] + bbox[1]]\n",
        "\n",
        "    cr_img = crop(img, bbox)\n",
        "    result = ocr.ocr(cr_img, cls=False, det=False)\n",
        "    ocr_res = result[0][0]\n",
        "    rec_conf = result[0][1]\n",
        "    print(result)\n",
        "    # Plotting the predictions using OpenCV.\n",
        "    (label_width,label_height), baseline = cv2.getTextSize(ocr_res , font, 2, 3)\n",
        "    top_left = tuple(map(int,[int(bbox[0]),int(bbox[1])-(label_height+baseline)]))\n",
        "    top_right = tuple(map(int,[int(bbox[0])+label_width,int(bbox[1])]))\n",
        "    org = tuple(map(int,[int(bbox[0]),int(bbox[1])-baseline]))\n",
        "\n",
        "    cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), blue_color, 2)\n",
        "    cv2.rectangle(img, top_left, top_right, blue_color,-1)\n",
        "    cv2.putText(img, ocr_res, org, font, 2, white_color,3)\n",
        "\n",
        "  # Writing output image.\n",
        "  file_name = os.path.join(out_path, 'out_' + input.split('/')[-1])\n",
        "  cv2.imwrite(file_name, img)"
      ],
      "metadata": {
        "id": "QW_WKW_Stog1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tracker_test_vid(vid_dir, config_file, weights,out_path):\n",
        "  # Declaring variables for video processing.\n",
        "  cap = cv2.VideoCapture(vid_dir)\n",
        "  codec = cv2.VideoWriter_fourcc(*'XVID')\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "  file_name = os.path.join(out_path, 'out_' + vid_dir.split('/')[-1])\n",
        "\n",
        "  out = cv2.VideoWriter(file_name, codec, fps, (width, height))\n",
        "\n",
        "  # Declaring variables for tracker.\n",
        "  max_cosine_distance = 0.4\n",
        "  nn_budget = None\n",
        "  \n",
        "  # Intializing tracker\n",
        "  model_filename = deepsort_model_file\n",
        "  encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
        "  metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
        "  tracker = Tracker(metric)\n",
        "  \n",
        "  # Initializing some helper variables.\n",
        "  ct = 0\n",
        "  preds = []\n",
        "  total_obj = 0\n",
        "  rec_tot_time = 1\n",
        "  alpha = 0.5\n",
        "  \n",
        "  # Loading darknet network and classes along with the bbox colors.\n",
        "  network, class_names, class_colors = darknet.load_network(\n",
        "          config_file,\n",
        "          data_file,\n",
        "          weights,\n",
        "          batch_size= batch_size\n",
        "      )\n",
        "  \n",
        "  # Reading video frame by frame.\n",
        "  while(cap.isOpened()):\n",
        "    ret, img = cap.read()\n",
        "    if ret == True:\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        print(ct)\n",
        "        \n",
        "        w_scale = w/1.55\n",
        "        h_scale = h/17\n",
        "\n",
        "        # Method to blend two images, here used to make the information box transparent.\n",
        "        overlay_img = img.copy()\n",
        "        cv2.rectangle(img, (int(w_scale), 0), (w, int(h_scale*3.4)), (0,0,0), -1)\n",
        "        cv2.addWeighted(img, alpha, overlay_img, 1 - alpha, 0, overlay_img)\n",
        "\n",
        "        # Noting time for calculating FPS.\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # Performing the YOLOv4 detection.\n",
        "        bboxes, scores, det_time = yolo_det(img, config_file, data_file, batch_size, weights, thresh, out_path, network, class_names, class_colors)\n",
        "        \n",
        "        if list(bboxes):\n",
        "          # Getting appearence features of the object.\n",
        "          features = encoder(img, bboxes)\n",
        "          # Storing all the required info in a list.\n",
        "          detections = [Detection(bbox, score, feature) for bbox, score, feature in zip(bboxes, scores, features)]\n",
        "\n",
        "          # Applying tracker.\n",
        "          # The tracker code flow: kalman filter -> target association(using hungarian algorithm) and appearance descriptor.\n",
        "          tracker.predict()\n",
        "          tracker.update(detections)\n",
        "          track_time = time.time() - prev_time\n",
        "          \n",
        "          # Checking if tracks exist.\n",
        "          for track in tracker.tracks:\n",
        "            if not track.is_confirmed() or track.time_since_update > 1:\n",
        "                continue\n",
        "\n",
        "            # Changing track bbox to top left, bottom right coordinates\n",
        "            bbox = list(track.to_tlbr())\n",
        "            \n",
        "            for i in range(len(bbox)):\n",
        "              if bbox[i] < 0:\n",
        "                bbox[i] = 0\n",
        "\n",
        "            # Extracting or cropping the license plate and applying the OCR.\n",
        "            cr_img = crop(img, bbox)\n",
        "            \n",
        "            rec_pre_time = time.time()\n",
        "            result = ocr.ocr(cr_img, cls=False, det=False)\n",
        "            rec_tot_time = time.time() - rec_pre_time\n",
        "\n",
        "            ocr_res = result[0][0]\n",
        "            print(result)\n",
        "            rec_conf = result[0][1]\n",
        "            \n",
        "            if rec_conf == 'nan':\n",
        "              rec_conf = 0\n",
        "\n",
        "            # Storing the ocr output for corresponding track id.\n",
        "            output_frame = {\"track_id\":track.track_id, \"ocr_txt\":ocr_res, \"ocr_conf\":rec_conf}\n",
        "            \n",
        "            # Appending track_id to list only if it does not exist in the list.\n",
        "            if track.track_id not in list(set(ele['track_id'] for ele in preds)):\n",
        "              total_obj = total_obj + 1\n",
        "              preds.append(output_frame)\n",
        "            # Looking for the current track in the list and updating the highest confidence of it.\n",
        "            else:\n",
        "              preds, rec_conf, ocr_res = get_best_ocr(preds, rec_conf, ocr_res, track.track_id)\n",
        "  \n",
        "            # Plotting the predictions using OpenCV.\n",
        "            txt = str(track.track_id) + '. ' + ocr_res\n",
        "            (label_width,label_height), baseline = cv2.getTextSize(txt , font,2,3)\n",
        "            top_left = tuple(map(int,[int(bbox[0]),int(bbox[1])-(label_height+baseline)]))\n",
        "            top_right = tuple(map(int,[int(bbox[0])+label_width,int(bbox[1])]))\n",
        "            org = tuple(map(int,[int(bbox[0]),int(bbox[1])-baseline]))\n",
        "\n",
        "            cv2.rectangle(overlay_img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), blue_color, 2)\n",
        "            cv2.rectangle(overlay_img, top_left, top_right, blue_color, -1)\n",
        "            cv2.putText(overlay_img,txt, org, font, 2, white_color, 3)\n",
        "            #cv2.imwrite('/content/{}.jpg'.format(ct), img)\n",
        "\n",
        "          # Calculating time taken and FPS for the whole process.\n",
        "          tot_time = time.time() - prev_time\n",
        "          fps = 1/tot_time\n",
        "          \n",
        "          # Writing information onto the frame and saving the frame to be processed into a video with title and values of different colors.\n",
        "          if w < 2000:\n",
        "            size = 1\n",
        "          else:\n",
        "            size = 2\n",
        "\n",
        "          # Plotting frame count information on the frame.\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Frame count:' , font,size,2)\n",
        "          top_left = (int(w_scale) + 10, int(h_scale))\n",
        "          cv2.putText(overlay_img, 'Frame count:', top_left, font, size, green_color, thickness=2)\n",
        "          \n",
        "          top_left_r1 = (int(w_scale) + 10 + label_width, int(h_scale))\n",
        "          cv2.putText(overlay_img,'%d ' % (ct), top_left_r1, font, size, yellow_color, thickness=2)\n",
        "\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Frame count:' + ' ' + str(ct) , font, size,2)\n",
        "          top_left_r1 = (int(w_scale) + 10 + label_width, int(h_scale))\n",
        "          cv2.putText(overlay_img, 'Total FPS:' , top_left_r1, font, size, green_color, thickness=2)\n",
        "\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Frame count:' + ' ' + str(ct) + 'Total FPS:' , font, size,2)\n",
        "          top_left_r1 = (int(w_scale) + 10 + label_width, int(h_scale))\n",
        "          cv2.putText(overlay_img, '%s' % (int(fps)), top_left_r1, font, size, yellow_color, thickness=2)\n",
        "\n",
        "          # Plotting Total FPS of ANPR information on the frame.\n",
        "          cv2.putText(overlay_img, 'Detection FPS:' ,(top_left[0], int(h_scale*1.7)), font, size, green_color, thickness=2)\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Detection FPS:', font,size,2)\n",
        "          cv2.putText(overlay_img, '%d' % ((int(1/det_time))),(top_left[0] + label_width, int(h_scale*1.7)), font, size, yellow_color, thickness=2)\n",
        "\n",
        "          # Plotting Recognition/OCR FPS of ANPR on the frame.\n",
        "          cv2.putText(overlay_img, 'Recognition FPS:',(top_left[0], int(h_scale*2.42)), font, size, green_color, thickness=2)\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Recognition FPS:', font,size,2)\n",
        "          cv2.putText(overlay_img, '%s' % ((int(1/rec_tot_time))),(top_left[0] + label_width, int(h_scale*2.42)), font, size, yellow_color, thickness=2)\n",
        "          cv2.imwrite('/content/{}.jpg'.format(ct), overlay_img)\n",
        "          out.write(overlay_img)\n",
        "        \n",
        "        # Increasing frame count.\n",
        "        ct = ct + 1\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "id": "C6izrc9buKwk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run single image"
      ],
      "metadata": {
        "id": "U7WWEWoNua5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img('/content/test.jpg', config_file, weights, '/content/')"
      ],
      "metadata": {
        "id": "u4lxB33zucQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run video (Still fail due to Tensorflow version related issue)"
      ],
      "metadata": {
        "id": "qoYaP0F7uix0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker_test_vid('test.mp4', config_file, weights, '/content/')"
      ],
      "metadata": {
        "id": "5max3w3uujzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}